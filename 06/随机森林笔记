day4 随机森林笔记和超参数的调参

Q : 如何选择最佳的超参数 ?
A : Python库函数gridsearchcv（网格参数寻优)
F :
param_grid = {"tol": [1e-4, 1e-3, 1e-2],
               "C": [0.4, 0.6, 0.8]}
log_reg = LogisticRegression(multi_class='ovr', solver='sag')
grid_search = GridSearchCV(log_reg, param_grid=param_grid, cv=3)
grid_search.fit(X, y)
利用这个grid_search 来代替log_reg ,进行模拟预测, 其中GridSearchCV 就会把本来打算做TrainData分成cv=3 ,三个部分,其中一部分用来做调参数据,一部分做训练数据,避免了动用了测试数据,导致后面模型对未来预测容错性低,由于每次选择的cv=3,这三部分都是随机选的,没有顺序,所以每次运行,结果可能会不同

Q : 什么是K折交叉验证? Cross Validation （交叉验证）
A : 学习器在测试集上的误差我们通常称作“泛化误差”。要想得到“泛化误差”首先得将数据集划分为训练集和测试集。那么怎么划分呢？常用的方法有两种，k折交叉验证法和自助法
注释:
(cross validation大概的意思是：对于原始数据我们要将其一部分分为train_data，一部分分为test_data。train_data用于训练，test_data用于测试准确率。在test_data上测试的结果叫做validation_error。将一个算法作用于一个原始数据，我们不可能只做出随机的划分一次train和test_data，然后得到一个validation_error，就作为衡量这个算法好坏的标准。因为这样存在偶然性。我们必须好多次的随机的划分train_data和test_data，分别在其上面算出各自的validation_error。这样就有一组validation_error，根据这一组validation_error，就可以较好的准确的衡量算法的好坏。
cross validation是在数据量有限的情况下的非常好的一个evaluate performance的方法。而对原始数据划分出train data和test data的方法有很多种，这也就造成了cross validation的方法有很多种。
sklearn中的cross validation模块，最主要的函数是如下函数：
sklearn.cross_validation.cross_val_score:他的调用形式是scores = cross_validation.cross_val_score(clf, raw_data, raw_target, cv=5, score_func=None)

参数解释：
clf:表示的是不同的分类器，可以是任何的分类器。比如支持向量机分类器。clf = svm.SVC(kernel=’linear’, C=1)；
raw_data：原始数据；
raw_target:原始类别标号；
cv：代表的就是不同的cross validation的方法了。引用scikit-learn上的一句话（When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.）如果cv是一个int数字的话，那么默认使用的是KFold或者StratifiedKFold交叉，如果如果指定了类别标签则使用的是StratifiedKFold。
cross_val_score:这个函数的返回值就是对于每次不同的的划分raw_data时，在test_data上得到的分类的准确率。至于准确率的算法可以通过score_func参数指定，如果不指定的话，是用clf默认自带的准确率算法。)

逻辑回归 : 是一种线性的有监督的分类模型
Q : 什么是决策树 ?   是一种非线性的有监督的分类模型
A : 决策树其实就是按节点分类数据集的一种方法
注解 : (决策树是一种常见的分类算法， 每一个叶子节点对应一个分类，非叶子节点对应某个属性的划分。决策树主要有3个部分组成，分别为决策节点，分支和叶子节点。其中决策树最顶端的节点为根决策点，每一个分支都有一个新决策点。决策节点下面是叶子节点。决策的过程从根决策点开始，从上到下。)
Q : 单颗决策树的缺点?
A : 1.运算量大,需要一次性加载全部数据进内存,并且找寻分割条件是一个极耗资源的工作
    2.训练样本中出现异常的数据,将会对决策树产生很大的影响,抗干扰能力差

Q : 如何解决上面单颗决策树的缺点 ?
A : 1.减少决策树所需的训练样本
    2.随机采样,降低异常数据的影响
和逻辑回归相对,逻辑回归告诉我们的是0-1的概率,而决策树只能告诉我们0 or 1


Q : 创建决策树进行分类的流程 ?
A : （1）    创建数据集
    （2）    计算数据集的信息熵
    （3）    遍历所有特征，选择信息熵最小的特征，即为最好的分类特征
    （4）    根据上一步得到的分类特征分割数据集，并将该特征从列表中移除
    （5）    执行递归函数，返回第三步，不断分割数据集，直到分类结束
    （6）    使用决策树执行分类，返回分类结果

Q : 什么是随机森林 ?  随机 :生成树的数据都是从数据集中随机选取的   森林 : 由树组成
A : 以决策树为基础的集合算法













