分类器评价指标主要有：
1，Accuracy
2，Precision
3，Recall
4，F1 score
5，ROC 曲线
6，AUC
7，PR 曲线
真正(True Positive , TP)：被模型预测为正的正样本。
假正(False Positive , FP)：被模型预测为正的负样本。
假负(False Negative , FN)：被模型预测为负的正样本。
真负(True Negative , TN)：被模型预测为负的负样本。
真正率(True Positive Rate,TPR)：TPR=TP/(TP+FN)，即被预测为正的正样本数 /正样本实际数。
假正率(False Positive Rate,FPR) ：FPR=FP/(FP+TN)，即被预测为正的负样本数 /负样本实际数。
假负率(False Negative Rate,FNR) ：FNR=FN/(TP+FN)，即被预测为负的正样本数 /正样本实际数。
真负率(True Negative Rate,TNR)：TNR=TN/(TN+FP)，即被预测为负的负样本数 /负样本实际数
准确率（Accuracy）
准确率是最常用的分类性能指标。
Accuracy = (TP+TN)/(TP+FN+FP+TN)，即正确预测的正反例数 /总数

精确率（Precision）
精确率容易和准确率被混为一谈。其实，精确率只是针对预测正确的正样本而不是所有预测正确的样本。表现为预测出是正的里面有多少真正是正的。可理解为查准率。
Precision = TP/(TP+FP)，即正确预测的正例数 /预测正例总数

召回率（Recall）
召回率表现出在实际正样本中，分类器能预测出多少。与真正率相等，可理解为查全率。
Recall = TP/(TP+FN)，即正确预测的正例数 /实际正例总数

F1 score
F值是精确率和召回率的调和值，更接近于两个数较小的那个，所以精确率和召回率接近时，F值最大。很多推荐系统的评测指标就是用F值的。
2/F1 = 1/Precision + 1/Recall

ROC曲线
逻辑回归里面，对于正负例的界定，通常会设一个阈值，大于阈值的为正类，小于阈值为负类。如果我们减小这个阀值，更多的样本会被识别为正类，提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了直观表示这一现象，引入ROC。根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve，横坐标为False Positive Rate(FPR假正率)，纵坐标为True Positive Rate(TPR真正率)。一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方

AUC
AUC（Area Under Curve）被定义为ROC曲线下的面积(ROC的积分)，通常大于0.5小于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。AUC值(面积)越大的分类器，性能越好

PR曲线
PR曲线的横坐标是精确率P，纵坐标是召回率R。评价标准和ROC一样，先看平滑不平滑（蓝线明显好些）。一般来说，在同一测试集，上面的比下面的好（绿线比红线好）。当P和R的值接近时，F1值最大，此时画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好像AUC对于ROC一样。一个数字比一条线更方便调型。


总结:
1、经验误差与过拟合
2、评估方法
    2.1 留出法
    2.2 交叉验证法
    2.3 自助法
3、性能度量
    3.1 错误率与精度
    3.2 混淆矩阵
    3.3 ROC与AUC
    3.4 均方误差

1、经验误差与过拟合
通常我们把分类错误的样本数占样本总数的比例称为“错误率”(error rate)，即如果在m个样本中有a个样本分类错误，则错误率E=a/m；相应的，1-a/m称为“精度”(accuracy)，即“精度=1一错误率”。更一般地，我(学习器的实际预测输出与样本的真实输出之间的差异称为“误差”(error)，学习器在训练集上的误差称为“训练误差”(training error)或“经验误差”(empirical error)，在新样本上的误差称为“泛化误差”(generalization error)。显然，我们希望得到泛化误差小的学习器，然而，我们事先并不知样本是什么样，实际能做的是努力使经验误差最小化。
“过拟合”（overfitting）与“欠拟合”（underfitting），学习器把训练样本学得太好了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会有的一般性质，这样就会导致泛化能力下降。这种现象在机器学习中称为过拟合，相对地，欠拟合是指对训练样本的一般性质尚未学好。
