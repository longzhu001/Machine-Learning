线性回归：
算法选择顺序，Ridge Regression (L2正则化) --> ElasticNet (即包含L1又包含L2) 
              --> Lasso Regression (L1正则化)

Q：正则化L1和L2有什么区别？
A：L1是w绝对值加和，L2是w平方加和。L1的有趣的现象是会使得w有的接近于0，有的接近于1，
   L1更多的会用在降维上面，因为有的是0有的是1，我们也称之为稀疏编码。
   L2是更常用的正则化手段，它会使得w整体变小

超参数alpha 在Rideg类里面就直接是L2正则的权重
超参数alpha 在Lasso类里面就直接是L1正则的权重
超参数alpha 在ElasticNet和SGDRegressor里面是损失函数里面的alpha
超参数l1_ration 在ElasticNet和SGDRegressor里面是损失函数的p

多项式回归：叫回归但并不是去做拟合的算法
PolynomialFeatures是来做预处理的，来转换我们的数据，把数据进行升维！

Q：升维有什么用？
A：升维就是增加更多的影响Y结果的因素，这样考虑的更全面，最终是要增加准确率！
   还有时候，就像PolynomialFeatures去做升维，是为了让线性模型去拟合非线性的数据！

Q：PolynomialFeatures是怎么升维的？
A：可以传入degree超参数，如果等于2，那么就会在原有维度基础之上增加二阶的数据变化！
   更高阶的以此类推

Q：如果数据是非线性的变化，但是就想用线性的模型去拟合这个非线性的数据，怎么办？
A：1，非线性的数据去找非线性的算法生成的模型去拟合
   2，可以把非线性的数据进行变化，变成类似线性的变化，然后使用线性的模型去拟合
      PolynomialFeatures类其实就是这里说的第二种方式

更高阶的数据变化，可以让线性模型拟合的更好，但是也容易过拟合！


保险的案例：
目的：未来来个新的人，可以通过模型来预测他的医疗花销
所以，就把charges列作为y，其他列作为X维度

Q：为什么每行没有人名？
A：人名不会对最终的Y结果产生影响，所以可以不用

Q：为什么要观测注意数据多样性，采样要均匀？
A：就是因为你要的模型的功能是对任何年龄段的人都有一个好的预测，那么你的模型在训练的时候
   读取的数据集，就得包含各个年龄段得数据，而且各个年龄段也得数据均匀，防止过拟合！

Q：什么是Pearson相关系数？
A：Pearson相关系数是来测量两组变量之间的线性相关性的！Pearson相关系数的区间范围是-1到1之间
   如果越接近于-1，说明两组变量越负相关，一个变大，另一个变小，反之如果越接近于1，说明两组
   变量越正相关，一个变大，另一个也跟着变大，如果越接近于0，说明越不相关，即一个变大或变小，
   另一个没什么影响！
   通过Pearson相关系数，如果发现两个维度之间，相关系数接近于1，可以把其中一个去掉，做到降维！
   通过Pearson相关系数，如果发现某个维度和结果Y之间的相关系数接近于0，可以把这个维度去掉，降维！

@:param corr()  列和列之间的相关性,利用是的Pearson(皮尔逊)相关系数(前提只能是数字数据)
    对于 自变量 和 自变量 之间的P
    P得出的结果 -1 <--> 1   -1 的是负相关  1 是正相关  0 不相关  ,值都是接近
    正相关,代表两个变量想类似,可以合并项或者去掉,达到降低维度的效果
    对于 因变量  和  自变量  之间的P
    若是正相关,证明他们之间的相关性好, 会因为自变量的变化,而导致因变量的大幅度变化