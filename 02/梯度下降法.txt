Q：梯度下降法是干嘛的？
A：梯度下降法是一种以最快的速度找到最优解的方法！

梯度下降法流程：
1，初始化theta，w0...wn
2，接着求梯度gradient
3，theta_t+1 = theta_t - grad * learning_rate
learning_rate是个超参数，太大容易来回振荡，太小步子太短，需要走很长时间，不管太大还是太小，
都会迭代次数很多，耗时很长
4，等待grad < threshold，迭代停止，收敛，threshold是个超参数

推导线性回归的loss function的导函数，目的是可以更快的求解梯度！
grad_j = (1/m) * (Xj)^Transpose * (X*theta - y)
grads = (1/m) * X^Transpose * (X*theta - y)

上面就是批量梯度下降的时候，去求解gradients梯度的公式！
不管是批量梯度下降，还是随机梯度下降，流程里面的1，3，4都是一样的，只有第二步求梯度稍有不同！

Q：随机梯度下降，怎么随机的呢？
A：其实就是在求梯度的时候，不再用所有的m个样本数据来计算，而是随机的选择一条数据来计算梯度！

Q：随机梯度下降的好处是什么？缺点是什么？
A：在求梯度的时候快，迭代次数有可能更多，最终可能落不到全局最优解上

Q：Mini-Batch GD是什么？
A：就是在求梯度的时候做了一个折中，不用所有的数据，而是随机选择一部分数据来求梯度！

上面代码里面除了随机抽取一条数据来求解梯度，还随着迭代次数的增多，不断减小步长！learning_rate

Q：为什么要不断的调整步长？
A：就是为了让越接近最优解的时候，调整的幅度越小，避免来回震荡！

Q：如果我们不人为的调小步长，会不会随着迭代的次数增多，调整的幅度自动减小？
A：调整的幅度取决于谁？却决于学习率和梯度，梯度事实上越接近最优解，梯度的绝对值越小

Q：为什么要做归一化？
A：只要是基于梯度来进行下降求解最优解，都需要归一化，目的是各个维度梯度可以同时收敛

Q：不做归一化，产生的问题是什么？
A：如果X1<<X2，那么W1>>W2，那么我们W1初始化之后要到达最优解的位置走的距离就远大于
   W1初始化之后要到达最优解的位置走的距离！
   因为X1<<X2，那么g1 = (y_hat-y)*x1 ，g2 = (y_hat-y) * x2，那么g1<<g2
   因为g1<<g2，那么W调整的幅度等于W_t+1 - W_t = - alpha * g
   所以g越小，调整的幅度就越小
   总结一下上面的推导：
   X1<<X2，W1调整的幅度<<W2调整的幅度，但是W1需要调整的距离>>W2需要调整的距离
   矛盾就产生了，如果此时不做归一化，去使用梯度下降求解最优解的话，
   产生的效果，即会是同样的迭代次数下，W2已经调整好了，W1还在慢慢的往前挪，
   整体看起来，就比先做归一化，再做梯度下降，需要的迭代次数要多了！！！

Q：怎么让多个维度对应的W基本上在同一时刻收敛？
A：对多个维度X来进行统一的归一化，比如说，最大值最小值归一化的方法

Q：何为最大值最小值归一化呢？
A：(X-Xmin)/(Xmax-Xmin)，最大值最小值归一化的特点是一定可以把一列数据归到0到1之间

Q：什么是过拟合？
A：拟合过度，用算法生成的模型，很好的拟合了你以有的数据，训练集数据，但是当来新的数据的时候，
   比如测试集的数据，预测的准确率反而降低了很多，那这个时候就是发生了过拟合现象

Q：如何防止过拟合呢？
A：防止过拟合，等价于提高模型的泛化能力，或者推广能力，或者说白了就是举一反三的能力！
   提高了模型的容错能力！
   学霸：有监督的机器学习！
   学神：有很强的学习能力，能自己找到学习的方法！无监督的机器学习！
   学渣：你的算法压根就没选对，数据预处理也没对，学习方法不对！
   学痴：做练习题都会，考试稍微一变化，就挂！过拟合了！没有泛化能力！

Q：如何在机器学习里面防止过拟合呢？
A：模型参数W个数，越少越好，无招胜有招
   模型参数W的值越小越好，这样如果X输入有误差，也不会太影响y预测结果
   通过正则化惩罚项人为的修改已有的损失函数，比如使用L1、L2正则添加到loss func里面去
   L1 = n个维度的w绝对值加和
   L2 = n个维度的w平方和
   让我们的SGD，在找最优解的过程中，考虑惩罚项的影响

Q：当使用惩罚项，会产生什么影响？
A：使用惩罚项，会提高模型的泛化能力，但是因为人为的改变了损失函数，所有在一定程度上牺牲了
   正确率，即对训练集已有数据的拟合效果，但是没关系，因为我们的模型目的是对未来新的数据进行预测
   在惩罚项里面，会有个alpha，即惩罚项的权重，我们可以通过调整alpha超参数，根据需求来决定
   是更看重模型的正确率还是模型的泛化能力！

